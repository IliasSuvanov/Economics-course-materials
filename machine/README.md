
# Machine Learning

# Content

- Machine Learning and Economics
- Intro to Python
- - Variables(text, digits)
- - Mulitline strings
- - List
- - Tuples
- - Dictionaries
- - Sets
- - If statements
- - For loops
- - Functions
- Matplotlib package
- Seaborn package
- Numpy package
- Pandas package
- Scikit-learn package
- - Linear regression
- - - Accuracy score (R^2)
- - - Train_test_split()
- - - Complexity of the model
- - - Lasso regression
- - - Ridge regression
- - Logistic regression
- - Support Vector Machine
- - Decision trees
- - Random forest
- - Xgboost
- - K-Fold cross validation


## About course
<p>Machine learning is used when you need to learn how to solve some class of problems for which it is difficult to write an explicit algorithm, but you can find many examples with correct answers.

So, it is impossible to preset yourself a handwritten algorithm that would be able to distinguish a photo of a cat from
pictures of the dog, but if you have enough pictures of both, you can use machine learning to build such an algorithm automatically.
In this course, students will learn about principles and algorithms for turning training data into effective automated predictions.

We will cover how to predict poverty scores, how to predict health outcomes for the people and so on.

**Instructor**: Ilias Suvanov

Lecuters will be held on Friday at 08:00

<h2><span class="mw-headline" id=".D0.9F.D0.BE.D0.BB.D0.B5.D0.B7.D0.BD.D1.8B.D0.B5_.D1.81.D1.81.D1.8B.D0.BB.D0.BA.D0.B8">Beneficial links</span></h2>
<ul>
	<li> <a rel="nofollow" class="external text" href="https://t.me/+YHDz7zMJZ6dhMzYy">Course chat
	</a>
	</li>
	<li> <a rel="nofollow" class="external text" href="https://zoom.us/j/92513119817?pwd=U3lEbVh1QjVPVmYvN0FtK0l0OHRTUT09">Zoom link
	</a>
	</li>
	<li> <a rel="nofollow" class="external text" href="">GitHub repositoy with course materials</a>
	</li>
	<li> <a rel="nofollow" class="external text" href="https://docs.google.com/forms/d/e/1FAIpQLSfLbATYt7fk3mtD0eK3eQSjcrmlhrBozAfKqcOdpeMRF3pHdw/viewform?usp=sf_link">Anonymous feedback</a>
	</li>
	<li> <a rel="nofollow" class="external text" href="https://docs.google.com/spreadsheets/d/1LqXnvP_nJlfSS4z8DBPtloJnbC1tX3rMAP0JZATS6WQ/edit#gid=0">Gradebook</a>
	</li>
	<li> <a rel="nofollow" class="external text" href="https://docs.google.com/document/d/18FfomLX9OqCaJrk8OvrPvjFov7YK_qm6E789SFXuOcA/edit">Grading Policy</a>
	</li>

</ul>
<h2><span class="mw-headline" id=".D0.A1.D0.B5.D0.BC.D0.B8.D0.BD.D0.B0.D1.80.D1.8B">Seminars</span></h2>
<table class="wikitable">

<tr>

	<th> Instructor </th>
	<th> Schedule</th>
</tr>
<tr>
	<td> <a rel="nofollow" class="external text" href="https://tlg.name/ilka2019">Ilias Suvanov</a> </td>
	<td> Lecuters will be held on Friday at 08:00</td>
</tr>
</table>

<h2><span class="mw-headline" id=".D0.9D.D0.BE.D0.B2.D0.BE.D1.81.D1.82.D0.B8_.D0.BA.D1.83.D1.80.D1.81.D0.B0">Course News</span></h2>
TBA
<br>

<h2><span class="mw-headline" id=".D0.9B.D0.B5.D0.BA.D1.86.D0.B8.D0.B8">Lectures/Seminars</span></h2>
<table class="wikitable">
<tr>
	<th> â„– </th>
	<th> Date</th>
	<th> Title </th>
	<th> Materials	</th>
	<th>View</th>
	<th>Additional Materials</th>
</tr>
<tr>
	<td> L1 </td>
	<td> 11th January </td>
	<td> What is Data, Why Python, Data-driven policy making.
        An introduction to machine learning. Basic terms, problem statements and application examples. </td>
    <td>   <a rel="nofollow" class="external text" href="https://drive.google.com/file/d/1mNXLRPaI8Ih9_1vOz372i8k594HJr4zR/view?usp=sharing">Slides</a></td>
	<td></td>
	<td></td>
</tr>

<tr>
	<td> S1 </td>
	<td> 11th January </td>
	<td> Variables, Strings, Multiline Strings in Python, If Statment, List, Dictionary, Set, For Loop Statement in Python </td>
	<td> <a rel="nofollow" class="external text" href="https://github.com/IliasSuvanov/MachineLearning/blob/main/Seminar1/Seminar1.ipynb">Seminar 1</a>
	</td>
	<td> <a rel="nofollow" class="external text" href="https://nbviewer.jupyter.org/github/IliasSuvanov/MachineLearning/blob/main/Seminar1/Seminar1.ipynb"> nbviewer</a>
	</td>
	<td>
		<a rel="nofollow" class="external text" href="https://www.youtube.com/playlist?list=PLBZBJbE_rGRWeh5mIBhD-hhDwSEDxogDg"> youtube tutorials</a>
	</td>
</tr>
<tr>
	<td> S2 </td>
	<td> 18th January </td>
	<td> Pandas, numpy and matplotlib library  </td>
	<td> <a rel="nofollow" class="external text" href="https://github.com/IliasSuvanov/MachineLearning/blob/main/Seminar2/Seminar2.ipynb">Seminar 2</a>
	</td>
	<td><a rel="nofollow" class="external text" href="https://nbviewer.jupyter.org/github/IliasSuvanov/MachineLearning/blob/main/Seminar2/Seminar2.ipynb">nbviewer</a>
		<a rel="nofollow" class="external text" href="https://youtu.be/TVSzMmOB25c">Video</a>
	</td>
	<td>  <a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=LHBE6Q9XlzI"> youtube tutorials</a>  </td>
</tr>

<tr>
	<td> S3 </td>
	<td> 25th January </td>
	<td> Guest lecture by Ilya Schurov. Intro to Machine Learning  </td>
	<td> 	</td>
	<td>  <a rel="nofollow" class="external text" href="https://youtu.be/iqG_mlbjzUY">Video</a> 	</td>
	<td></td>
</tr>

<tr>
	<td> S4 </td>
	<td> 1st February </td>
	<td> Linear Regression  </td>
	<td> 	</td>
	<td>   	</td>
	<td>
		<a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=nk2CQITm_eo">Theory</a>
		<a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=8jazNUpO3lQ&t=819s">Code</a>
		<a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=J_LnPL3Qg70">Code</a>

	</td>
</tr>

<tr>
	<td> S5 </td>
	<td> 8th February </td>
	<td>  Train/Test Split </td>
	<td> 	</td>
	<td>  	</td>
	<td> <a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=fwY9Qv96DJY&list=RDCMUCh9nVJoWXmFb7sLApWGcLPQ&index=9">Code</a> </td>
</tr>


<tr>
	<td> S6 </td>
	<td> 15th February </td>
	<td>  Presentation: Machine Learning for Everyone  </td>
	<td> 	</td>
	<td> 	</td>
	<td></td>
</tr>

<tr>
	<td> S7 </td>
	<td> 22nd February </td>
	<td>  Logistic Regression </td>
	<td> 	</td>
	<td>   	</td>
	<td></td>
</tr>



<tr>
	<td> S8 </td>
	<td> 1st March </td>
	<td> Logistic Regression  </td>
	<td> 	</td>
	<td>  	</td>
	<td>
		<a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=yIYKR4sgzI8&t=98s">Theory</a>
		<a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=zM4VZR0px8E">Code</a>
		<a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=J5bXOOmkopc&t=719s">Code</a>
	</td>
</tr>

<tr>
	<td> S9 </td>
	<td> 15th March </td>
	<td>  Support Vector Machine (SVM) </td>
	<td> 	</td>
	<td>   	</td>
	<td>
		<a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=Y6RRHw9uN9o">SVM Theory</a>
		<a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=7sz4WpkUIIs">SVM Code</a>
	</td>
</tr>

<tr>
	<td> S10 </td>
	<td> 29th March </td>
	<td> K-Fold Cross Validation. L1 and L2 regularization.  </td>
	<td> 	</td>
	<td> 	</td>
	<td><a rel="nofollow" class="external text" href="https://www.youtube.com/results?search_query=k-fold+cross+validation">K-fold</a>
		<a rel="nofollow" class="external text" href="https://www.youtube.com/results?search_query=k-fold+cross+validation">K-fold</a>  </td>
</tr>

<!-- <tr>
	<td> S3 </td>
	<td> TBA </td>
	<td> Functions, Modules, Matplotlib library, Stock prices plots  </td>
	<td> <a rel="nofollow" class="external text" href="https://github.com/IliasSuvanov/DSforEconomists/blob/master/Seminar3/Seminar3.ipynb">Seminar 3</a>
	</td>
	<td><a rel="nofollow" class="external text" href="https://nbviewer.jupyter.org/github/IliasSuvanov/DSforEconomists/blob/master/Seminar3/Seminar3.ipynb">nbviewer</a></td>
	<td></td>
</tr> -->
<!-- <tr>
	<td> S4 </td>
	<td> TBA </td>
	<td> Introduction to Numpy and Pandas library </td>
	<td> <a rel="nofollow" class="external text" href="https://github.com/IliasSuvanov/DSforEconomists/blob/master/Seminar4/Seminar4.ipynb">Seminar 4</a>
	</td>
	<td><a rel="nofollow" class="external text" href="https://nbviewer.jupyter.org/github/IliasSuvanov/DSforEconomists/blob/master/Seminar4/Seminar4.ipynb">nbviewer</a></td>
	<td></td>
</tr>

<tr>
	<td> S5 </td>
	<td> TBA </td>
	<td> GroupBy, Pivot and Pivot Table methods</td>
	<td> <a rel="nofollow" class="external text" href="https://github.com/IliasSuvanov/DSforEconomists/blob/master/Seminar5/Seminar5.ipynb">Seminar 5</a>
	</td>
	<td><a rel="nofollow" class="external text" href="https://nbviewer.jupyter.org/github/IliasSuvanov/DSforEconomists/blob/master/Seminar5/Seminar5.ipynb">nbviewer</a></td>
	<td></td>
</tr>

<tr>
	<td> L2 </td>
	<td> TBA </td>
	<td> Linear regression: training and different error functionals. Gradient descent. </td>
    <td>  <a rel="nofollow" class="external text" href="https://github.com/esokolov/ml-course-hse/blob/master/2016-fall/lecture-notes/lecture02-linregr.pdf">Slides</a>
	</td>
	<td></td>
	<td></td>
</tr>

<tr>
	<td> L3 </td>
	<td> TBA </td>
	<td> Regularization. Methods for assessing generalizing ability, cross-validation. </td>
    <td>  <a rel="nofollow" class="external text" href="https://github.com/esokolov/ml-course-hse/blob/master/2016-fall/lecture-notes/lecture03-linregr.pdf">Slides</a>
	</td>
	<td></td>
	<td></td>
</tr>

<tr>
	<td> L4 </td>
	<td> TBA </td>
	<td> Approximation of empirical risk. Probability estimation problem, logistic regression. The idea of calibration of probabilities. Perceptron. Quality metrics in classification problems. </td>
    <td>  <a rel="nofollow" class="external text" href="https://github.com/esokolov/ml-course-hse/blob/master/2016-fall/lecture-notes/lecture04-linclass.pdf">Slides</a>
	</td>
	<td></td>
	<td></td>
</tr>

<tr>
	<td> L5 </td>
	<td> TBA </td>
	<td> Support vector machine, its dual problem (without kernels). Generalized linear models. Problem statement for multiclass and multilabel classification. </td>
    <td>  <a rel="nofollow" class="external text" href="https://github.com/esokolov/ml-course-hse/blob/master/2016-fall/lecture-notes/lecture05-linclass.pdf">Slides</a>
	</td>
	<td></td>
	<td></td>
</tr>

<tr>
	<td> L6 </td>
	<td> TBA </td>
	<td> Decision trees, random forests. </td>
    <td>  <a rel="nofollow" class="external text" href="https://github.com/esokolov/ml-course-hse/blob/master/2017-fall/lecture-notes/lecture07-trees.pdf">Slides</a>
        <a rel="nofollow" class="external text" href="https://github.com/esokolov/ml-course-hse/blob/master/2017-fall/lecture-notes/lecture07-trees.pdf">Slides</a>
	</td>
	<td></td>
	<td></td>
</tr> -->





</table>


<p><a rel="nofollow" class="external text" href="https://stackoverflow.com/questions/45622602/how-to-jupyter-notebooks-from-github">How to download .ipynb file from GitHub?</a>
</p>


        <h2><span class="mw-headline" id="grading_policy">Grading Policy</span></h2>
		<p>Final Grade of course participants will be measured by the number of completed homeworks and FINAL exam.
		Additional points will be given to course participants for the participation in the class, presentations given in the class etc.</p>

		<h2><span class="mw-headline" id=".D0.9A.D0.BE.D0.BD.D1.82.D1.80.D0.BE.D0.BB.D1.8C.D0.BD.D1.8B.D0.B5_.D1.80.D0.B0.D0.B1.D0.BE.D1.82.D1.8B">Control Work</span></h2>
        <h3><span class="mw-headline" id="Midterm">Midterm</span></h3>
        	<p>There would be NO MIDTERM exam for this course.</p>
		<h3><span class="mw-headline" id="Midterm">Oral Examination</span></h3>
			<p>Instructor reserves the right to require any course participant to sit for an individual oral examination (with turned on webcam and mic) before submitting the final grade to the registrar.
			Refusal to sit for an individual oral examination by course participant, may result in failing the course.
			Instructor will notify potential course participants about oral examination in the mid of April.
			</p>




		<h3><span class="mw-headline" id="Final">Final</span></h3>
			<p>For the FINAL exam you need to do an individual(groups are not allowed) analysis on the Kaggle platform, using any dataset on the platform.
				Your analysis should include Exploratory Data Analysis, Linear Regression model, Logistic Regression model and accuracy scores.
				You need to present your analysis in the class. Presentation should be no more than 5 minutes. The schedule for presentations can be found under the link https://docs.google.com/spreadsheets/d/1LqXnvP_nJlfSS4z8DBPtloJnbC1tX3rMAP0JZATS6WQ/edit#gid=0
			</p>
	<h2><span class="mw-headline" id=".D0.9F.D0.BE.D0.BB.D0.B5.D0.B7.D0.BD.D1.8B.D0.B5_.D0.BC.D0.B0.D1.82.D0.B5.D1.80.D0.B8.D0.B0.D0.BB.D1.8B">Learning Python Materials</span></h2>
	<h3><span class="mw-headline" id=".D0.91.D0.B0.D0.B7.D0.BE.D0.B2.D1.8B.D0.B5_.D1.83.D1.87.D0.B5.D0.B1.D0.BD.D0.B8.D0.BA.D0.B8">Foundational books</span></h3>
		<ol>
			<li>Ben Stephenson - The Python Workbook: A Brief Introduction with Exercises and Solutions</li>
			<li>Nicola Lacey - Python by Example: Learning to Program in 150 Challenges</li>
			<li>Python documentation</li>
			<li>Jake VanderPlas - Python Data Science Handbook: Essential Tools for Working with Data</li>
			<li>Wes McKinney - Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython</li>
			<li>Joel Grus - Data Science from Scratch: First Principles with Python</li>
		</ol>

<h2><span class="mw-headline" id=".D0.9F.D0.BE.D0.BB.D0.B5.D0.B7.D0.BD.D1.8B.D0.B5_.D0.BC.D0.B0.D1.82.D0.B5.D1.80.D0.B8.D0.B0.D0.BB.D1.8B">Machine Learning Materials</span></h2>
<h3><span class="mw-headline" id=".D0.91.D0.B0.D0.B7.D0.BE.D0.B2.D1.8B.D0.B5_.D1.83.D1.87.D0.B5.D0.B1.D0.BD.D0.B8.D0.BA.D0.B8">Online courses</span></h3>
<ol>
	<li><a rel="nofollow" class="external text" href="http://wiki.cs.hse.ru/%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BD%D0%B0_%D0%BC%D0%B0%D1%82%D1%84%D0%B0%D0%BA%D0%B5_2020">Ilya Schurov - Machine Learning at HSE(Russian language)</a></li>
	<li><a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=njKP3FqW3Sk&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI">MIT Introduction to Deep Learning</a></li>
	<li>Andrew Ng - Machine learning on Coursera</li>
	<li><a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=HcqpanDadyQ&list=PLIivdWyY5sqJxnwJhe3etaK7utrBiPBQ2&index=1">Google - AI Adventures</a></li>
</ol>
<h3><span class="mw-headline" id=".D0.91.D0.B0.D0.B7.D0.BE.D0.B2.D1.8B.D0.B5_.D1.83.D1.87.D0.B5.D0.B1.D0.BD.D0.B8.D0.BA.D0.B8">Books</span></h3>
	<ol>
		<li> <a rel="nofollow" class="external text" href="https://mml-book.github.io">Mathematics for Machine Learning</a> - book with mathematical introduction to machine learning. You might be especially interested in Probability theory chapters.
		</li>
	</ol>
<h2><span class="mw-headline" id=".D0.9F.D0.BE.D0.BB.D0.B5.D0.B7.D0.BD.D1.8B.D0.B5_.D0.BC.D0.B0.D1.82.D0.B5.D1.80.D0.B8.D0.B0.D0.BB.D1.8B">Learning Statistics Materials</span></h2>
<h3><span class="mw-headline" id=".D0.91.D0.B0.D0.B7.D0.BE.D0.B2.D1.8B.D0.B5_.D1.83.D1.87.D0.B5.D0.B1.D0.BD.D0.B8.D0.BA.D0.B8">Online courses</span></h3>
<ol>
	<li><a rel="nofollow" class="external text" href="https://www.openintro.org/book/os/">OpenIntro Statistics</a></li>
	<li><a rel="nofollow" class="external text" href="https://www.khanacademy.org/math/statistics-probability">Khan Academy Statistics and probability</a></li>

</ol>

<h3><span class="mw-headline" id=".D0.91.D0.B0.D0.B7.D0.BE.D0.B2.D1.8B.D0.B5_.D1.83.D1.87.D0.B5.D0.B1.D0.BD.D0.B8.D0.BA.D0.B8">Books</span></h3>
	<ol>
		<li>Christopher Barr, David M. Diez, and Mine Ã‡etinkaya-Rundel - OpenIntro Statistics</li>

	</ol>


<h3><span class="mw-headline" id=".D0.A0.D0.B0.D0.B7.D0.BD.D1.8B.D0.B5_.D1.85.D0.BE.D1.80.D0.BE.D1.88.D0.B8.D0.B5_.D1.81.D1.81.D1.8B.D0.BB.D0.BA.D0.B8">Miscellaneous beneficial links</span></h3>
<ol>
<li><a rel ="nofollow" class="external text" href="https://www.w3schools.com/python/">Online practical exercises</a></li>
</ol>
